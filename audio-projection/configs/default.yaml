# Default configuration for audio projection adapter training

# Model configuration
model:
  llm:
    name: "meta-llama/Meta-Llama-3.1-8B-Instruct"
    hidden_size: 4096
    freeze: true
  
  tts:
    name: "Qwen/Qwen3-TTS-12Hz-1.7B-Base"
    hidden_size: 2048
    freeze: true
  
  projector:
    dropout: 0.1
    use_pooling: false
    pooling_type: "avg"  # avg, max, or attention

# Training configuration
training:
  batch_size: 4
  gradient_accumulation_steps: 4
  num_epochs: 10
  max_steps: 100000
  
  learning_rate: 1e-4
  warmup_steps: 1000
  weight_decay: 0.01
  
  mixed_precision: "fp16"  # fp16, bf16, or null
  gradient_checkpointing: true
  
  # Loss weights
  token_loss_weight: 1.0
  perceptual_loss_weight: 0.1

# Data configuration
data:
  dataset_name: "librispeech_asr"
  dataset_config: "clean"
  train_split: "train.clean.100"
  eval_split: "validation.clean"
  
  max_audio_length: 30.0  # seconds
  sample_rate: 24000
  
  preprocessing:
    normalize_audio: true
    remove_silence: true
    min_duration: 1.0
    max_duration: 30.0

# Optimization
optimization:
  optimizer: "adamw"  # adamw or adam
  scheduler: "cosine"  # cosine, linear, or constant
  
  # AdamW parameters
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-8
  
  # Gradient clipping
  max_grad_norm: 1.0

# Evaluation
evaluation:
  eval_steps: 1000
  save_steps: 5000
  logging_steps: 100
  
  eval_batch_size: 8
  num_eval_samples: 500

# Logging
logging:
  use_wandb: false
  wandb_project: "audio-projection"
  wandb_entity: null
  
  output_dir: "./outputs"
  checkpoint_dir: "./checkpoints"
  log_dir: "./logs"

# Hardware
hardware:
  device: "cuda"
  num_workers: 4
  pin_memory: true
  
  # Multi-GPU training
  distributed: false
  world_size: 1
  local_rank: -1
