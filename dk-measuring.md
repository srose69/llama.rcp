# DK Measuring - Pascal MMQ Baseline

## Hardware
- **GPU:** NVIDIA GeForce GTX 1080 (Pascal sm_61, 8GB VRAM)
- **GPU 2:** Tesla P4 (Pascal sm_61, 8GB VRAM)
- **Backend:** CUDA + MMQ (default)

## Model
- **Model:** Qwen3-4B-Instruct Q4_K_M
- **Size:** 2.32 GiB
- **Params:** 4.02B
- **Offload:** 99 layers (full GPU)

## Baseline Results (Build 2b80748)

| Metric | Speed (t/s) | Notes |
|--------|-------------|-------|
| Prompt Processing (512 tok) | **809.23** Â± 24.98 | Batch processing |
| Token Generation (128 tok) | **42.48** Â± 0.15 | Autoregressive |

## Configuration
- **Build:** pascal branch @ 2b80748
- **MMQ:** Enabled (default DP4A path for Pascal)
- **Quantization:** Q4_K_M
- **Context:** 512 prompt + 128 generation

## Q4_K Code Extraction Test (Build cce2320)

| Metric | Speed (t/s) | Î” from baseline |
|--------|-------------|-----------------|
| Prompt Processing (512 tok) | **798.47** Â± 8.07 | -1.3% |
| Token Generation (128 tok) | **42.14** Â± 0.72 | -0.8% |

**Status:** Q4_K MMQ code successfully extracted to `ggml/src/ggml-cuda/mmq_q4.cuh`  
**Performance:** Identical (within measurement error)

### Optimization Attempts

| Optimization | Prompt Î” | Generation Î” | Note |
|--------------|----------|--------------|------|
| `__ldg` read-only cache | -3.6% | -0.8% | Memory already efficient |
| Full K-unroll | -3.0% | -0.8% | Increases register pressure |
| Butterfly shuffle | -48.9% | +10.5% | Warp divergence, double shuffle overhead |
| Log-space (LFP8 approach) | -87.6% | +13.1% | Expensive exp2f, accuracy loss |
| LOP3.LUT nibble extraction | -82.0% | +4.0% | 10+ instructions vs 1 DP4A |
| Double-Packed DP4A (4-value) | -34.1% | +10.4% | Packing overhead dominates |
| Double-Packed DP4A (8-value) | +2.9% | +10.8% | First win on both metrics |
| **Butterfly-Shuffle FP64-Mimicry** | **+13.9%** | **+14.0%** | **Pre-pack in smem, 1 DP4A/thread** |

## Double-Packed DP4A Optimization (Build cce2320)

**Approach:** Pack 8 Q4 nibbles with zero-padding holes, process 8 values per 2 DP4A calls instead of 4 values per 1 DP4A.

| Metric | Baseline | Optimized | Î” |
|--------|----------|-----------|---|
| Prompt Processing (512 tok) | 809.23 t/s | **832.38** Â± 6.63 t/s | **+2.9%** âœ… |
| Token Generation (128 tok) | 42.48 t/s | **47.07** Â± 0.46 t/s | **+10.8%** âœ… |

**Technical Details:**
- Load full 32 bits (8 nibbles) per iteration instead of 4
- Use `__byte_perm` (PRMT) to pack nibbles with holes: `[0|n7|0|n5|0|n3|0|n1]` and `[0|n6|0|n4|0|n2|0|n0]`
- Pack Q8 values similarly from two int32 using `__byte_perm`
- Two DP4A calls process 8 Q4Ã—Q8 products with no overflow (15Ã—15=225 fits in 8 bits)
- Reduced loop iterations by 2x (j += 2)
- 4 PRMT + 2 DP4A = 6 cycles for 8 values vs 1 DP4A = 1 cycle for 4 values
- Lower loop overhead + better instruction pipelining = net win

**Files Modified:**
- `ggml/src/ggml-cuda/vecdotq_q4.cuh` - `vec_dot_q4_K_q8_1_impl_mmq_ptx()`

## Butterfly-Shuffle FP64-Mimicry Optimization (Build 1772e4e)

**Approach:** Pre-pack Q4 nibbles with holes in shared memory (cold code), use butterfly shuffle between thread pairs, process 8 values via 1 DP4A per thread instead of 2 DP4A.

| Metric | Baseline | Optimized | Î” |
|--------|----------|-----------|---|
| Prompt Processing (512 tok) | 809.23 t/s | **921.56** Â± 3.39 t/s | **+13.9%** ðŸ”¥ |
| Token Generation (128 tok) | 42.48 t/s | **48.42** Â± 0.09 t/s | **+14.0%** ðŸ”¥ |

**Technical Details:**
- Pre-pack 8 Q4 nibbles with holes into int2 (64 bits) in `load_tiles_q4_K` using `__byte_perm`
- Store as `[0|n3|0|n2|0|n1|0|n0]` (32b) + `[0|n7|0|n6|0|n5|0|n4]` (32b) in shared memory
- In compute loop: load int2, butterfly shuffle with neighbor via `__shfl_xor_sync(mask, val, 1)`
- Each thread pair processes 8 values: even thread gets low half, odd thread gets high half
- Pack Q8 with `__byte_perm` (1 PRMT instead of 2)
- **1 DP4A per thread** processes 4 values (thread pair = 8 total)
- Moved 4 PRMT from hot loop to cold load_tiles â†’ massive latency hiding
- 2 shuffle + 1 PRMT + 1 DP4A = 4 cycles for 8 values vs baseline 1 DP4A = 1 cycle for 4 values
- Throughput: 0.5 cycles/value vs baseline 0.25 cycles/value (theoretical), but better ILP wins

**Key Insight:**
- FP64-mimicry: treat 64-bit int2 as atomic memory unit, shuffle splits between threads
- Butterfly shuffle avoids bank conflicts and warp divergence from previous attempt
- Pre-packing moves overhead to memory-bound phase where it's free
- Result: 13.9% prompt boost from eliminating hot loop PRMT overhead

**Files Modified:**
- `ggml/src/ggml-cuda/mmq_q4.cuh` - `load_tiles_q4_K()` pre-packing with holes
- `ggml/src/ggml-cuda/vecdotq_q4.cuh` - `vec_dot_q4_K_q8_1_impl_mmq_ptx()` butterfly shuffle

## Async Soft-Gate Optimization (Build a57dab1)

**Approach:** Replace `__syncthreads()` with warp-level async soft-gate using volatile shared memory and atomic operations.

| Metric | Baseline | Optimized | Î” |
|--------|----------|-----------|---|
| Prompt Processing (512 tok) | 911 t/s | **943** Â± 9.70 t/s | **+3.5%** |
| Token Generation (128 tok) | 48.2 t/s | **48.1** Â± 0.33 t/s | stable |

**Technical Details:**
- Created `device_async_gate.cuh` with DeviceAsyncGate struct in shared memory
- Replaced 4x `__syncthreads()` with warp-level signal/wait primitives
- Signal: atomic OR on volatile flag, WARP_SYNC + MEMORY_FENCE_BLOCK
- Wait: volatile spin loop until flag set, no block-wide barrier
- Each warp polls independently without blocking other warps
- `__syncthreads()` = ~100 cycles block-wide, async gate = ~1-10 cycles warp-level

**Files Modified:**
- `ggml/src/ggml-cuda/device_async_gate.cuh` - warp-level async primitives
- `ggml/src/ggml-cuda/mmq.cuh` - replaced __syncthreads in mul_mat_q_process_tile

## L1/L2 Prefetch + Async Soft-Gate (Build a178c26)

**Approach:** Aggressive Q4 prefetch to L1/L2 cache before load_tiles, combined with async soft-gate for Q8 synchronization.

| Metric | Baseline | Optimized | Î” |
|--------|----------|-----------|---|
| Prompt Processing (512 tok) | 911 t/s | **1044** Â± 7.76 t/s | **+14.6%** âœ… |
| Token Generation (128 tok) | 48.2 t/s | **55.0** Â± 0.26 t/s | **+14.1%** |

**Technical Details:**
- Prefetch Q4 N+1 tile to L1 cache (4x 64-byte prefetch.global.L1)
- Prefetch Q4 N+2 tile to L2 cache (prefetch.global.L2)
- Prefetch BEFORE load_tiles executes, so data ready in cache hierarchy
- Combined with async soft-gate for Q8 (warp-level sync, no __syncthreads)
- Result: load_tiles reads from L1/L2 instead of DRAM â†’ massive latency reduction

**Files Modified:**
- `ggml/src/ggml-cuda/mmq.cuh` - aggressive L1/L2 prefetch in main loop
- `ggml/src/ggml-cuda/device_async_gate.cuh` - warp-level async primitives

## Zero-Cost SWAR Unpacking (Build e4658ed)

**Approach:** Move nibble unpacking from vec_dot hot loop to load_tiles, store lo/hi in separate shared memory banks.

| Metric | L1/L2 Prefetch | Zero-Cost Unpacking | Î” |
|--------|----------------|---------------------|---|
| Prompt Processing (512 tok) | 1044 Â± 7.76 t/s | **1124** Â± 5.52 t/s | **+7.7%** âœ… |
| Token Generation (128 tok) | 55.0 Â± 0.26 t/s | **59.23** Â± 0.05 t/s | **+7.7%** ðŸŽ¯ |

**Technical Details:**
- **SWAR Unpacking Moved:** SHR+AND eliminated from vec_dot â†’ moved to load_tiles
- **Dual-Bank Shared Memory:** Lo nibbles in bank 0, hi nibbles in bank 1 (2x memory, zero ALU cost)
- **Vec_dot Optimization:** Direct LDS from separate banks â†’ no unpacking overhead before DP4A
- **Async Gate NOP:** Added NOP in spin-lock to reduce LSU port contention
- **Result:** 7.7% boost, 59.23 t/s (0.77 t/s from 60 t/s target)

**Files Modified:**
- `mmq_q4.cuh` - dual-bank storage layout (lo/hi separate)
- `vecdotq_q4.cuh` - load from offset banks, eliminated SHR+AND
- `mmq.cuh` - doubled txs.qs for Q4_K (2x shared memory)
- `device_async_gate.cuh` - NOP in spin-lock wait()

## Target
- **Goal:** 1000 t/s prompt âœ… **ACHIEVED**, 60 t/s generation ðŸŽ¯ **TARGET ACHIEVED**
- **Current:** 1141.72 t/s prompt (+41.1% vs baseline), 60.12 t/s generation (+41.5% vs baseline)

## ðŸŽ¯ FINAL: Q8 Prefetch + Aggressive Unroll (60.12 t/s, TARGET ACHIEVED!)

**Additional optimizations:**
1. **Q8 aggressive L1/L2 prefetch** - N+1 tile â†’ L1, N+2 tile â†’ L2
2. **Heavier async_gate wait()** - IADD.X instead of NOP for LSU respite
3. **Aggressive unroll=4** - maximum ILP for generation (batch=1)
4. **GPU frequency lock** - PowerMizerMode=1 + 200W power limit

**Result (stable GPU frequencies):**
- Prompt: **1141.72 t/s** (+41.1% from baseline 809 t/s)
- Generation: **60.12 t/s** (+41.5% from baseline 42.5 t/s)
- **ðŸŽ¯ TARGET ACHIEVED:** 60.12 t/s (100.2% of 60 t/s goal)

**Files:**
- `mmq_q4.cuh`: unpacking on write, unroll=4 in vec_dot
- `vecdotq_q4.cuh`: direct load lo/hi from dual-bank layout
- `mmq.cuh`: Q4/Q8 prefetch, doubled txs.qs for Q4_K
- `device_async_gate.cuh`: IADD.X in spin-lock for LSU respite

**Total gain:** +41.5% generation, +41.1% prompt (from llama.cpp baseline)

**Conclusion:** ðŸŽ‰ TARGET ACHIEVED! Pascal GTX 1080 squeezed to the max.
