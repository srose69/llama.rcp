# Default build configuration for llama.rcp
# Format: KEY=VALUE

# Build type: Debug, Release, RelWithDebInfo, MinSizeRel
BUILD_TYPE=Release

# Enable CUDA support
GGML_CUDA=ON

# CUDA architectures (comma-separated, e.g., "61,75,86")
# Common architectures:
#   60 - Pascal (GTX 10 series)
#   61 - Pascal (GTX 10 series, Titan Xp)
#   70 - Volta (V100)
#   75 - Turing (RTX 20 series, GTX 16 series)
#   80 - Ampere (A100)
#   86 - Ampere (RTX 30 series)
#   89 - Ada Lovelace (RTX 40 series)
#   90 - Hopper (H100)
CUDA_ARCHITECTURES=61

# Number of parallel jobs (0 = auto-detect)
PARALLEL_JOBS=0

# Enable additional backends (ON/OFF)
GGML_METAL=OFF
GGML_VULKAN=OFF
GGML_OPENCL=OFF
GGML_BLAS=OFF
GGML_RPC=OFF

# Build options
BUILD_SHARED_LIBS=ON
LLAMA_BUILD_TESTS=OFF
LLAMA_BUILD_EXAMPLES=ON
LLAMA_BUILD_SERVER=ON
